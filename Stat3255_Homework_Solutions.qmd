---
title: "Stat3255 Homework Solutions"
author: "Yifan Li"
format:
  html:
    code-fold: false
execute:
  warning: false
  error: false
jupyter: python3
---

## Q3: The Monty Hall Problem




Solution from Bedard Kaitlyn.


```{python}
import random
 
# function to choose the door the host opens
def open_door(win, choice, doors):
    # set containing doors that the host cannot open
    do_not_open = {win, choice} 
    # set containing all door indexes
    all_doors = set(doors) 
    # list containing all the doors that the host can open
    options = list(all_doors - do_not_open) 
    # choose a random door for the host to open
    opened = random.choice(options) 
    return opened
# function to choose the door the player switches to
def switch_doors(choice, opened_door, doors):
    # set containing the doors that the player cannot open
    cannot_open = {choice, opened_door} 
    # set containing all door indexes
    all_doors = set(doors) 
    # list containing the doors that the player can choose
    options = list(all_doors - cannot_open) 
    # choose a random door for the player to switch to 
    switched_door = random.choice(options)  
    return switched_door
# function to determine if staying results in win or loss
def stay(win, choice):
    # if chosen door is the winning door, win = True
    if win == choice:   
        return 1
    # else, chosen door is not winning door so, win = False
    else:               
        return 0
# function to determine if switching results in win or loss
def switch(win, choice, opened_door, doors):
    # must choose a random door to switch to if there are more then 3 doors
    if (len(doors) != 3):   
        switched_door = switch_doors(choice, opened_door, doors)
        # if newly chosen door is winning door, win = True
        if switched_door == win:    
            return 1
        # if newly chosen door is not winning door, win = False
        else:                      
            return 0
    # if ndoors is 3, we do not need to compute the door to switch to 
    # if initially chosen door is the prize door, switching results in loss 
    elif (win == choice):  
        return 0
    # otherwise initially chosen door is not prize, thus switching will win 
    else:                   
        return 1
# simulation function
def montyhall(ndoors, ntrials):
    # initialize our counts to 0
    switch_win_count = 0       
    stay_win_count = 0 
    # do ntrials
    for n in range(ntrials):  
        # create a list of doors (indexs start at 0)
        doors = [i for i in range(ndoors)]  
        # choose a winning door randomly
        win = random.choice(doors)  
        # player choses any random door
        choice = random.choice(doors) 
        # host opens a new door (without prize behind it)
        opened_door = open_door(win, choice, doors) 
        # if stay results in a win (1), increase stay win count
        if stay(win, choice):  
            stay_win_count += 1
        
        # if switch results in a win (1), increase switch win count
        if switch(win, choice, opened_door, doors): 
            switch_win_count += 1
    # average of winning if player stays
    stay_result = stay_win_count/ntrials   
    # average of winning if player switched    
    switch_result = switch_win_count/ntrials    
    return (stay_result, switch_result)
# test cases
print("The proportion of winning if the player stays in the 3 door " 
      "scenario is", montyhall(3,1000)[0])
print("The proportion of winning if the player switches in the 3 door " 
      "scenario is", montyhall(3,1000)[1])
print("The proportion of winning if the player stays in the 5 door "
      "scenario is", montyhall(5,1000)[0])
print("The proportion of winning if the player switches in the 5 door " 
      "scenario is", montyhall(5,1000)[1])
```



## Q4: Using Monte Carlo to Approximate Pi


Solution from Bedard Kaitlyn.


```{python}
import numpy as np
import scipy.stats as st

def pi_approx(n):
    # set random seed for replication
    np.random.seed(77) 
    # array of size n of random floats in range [0,1)                    
    x = np.random.random(n)   
    # array of size n of random floats in range [0,1)            
    y = np.random.random(n)               
    # array where 1 represents a point that lies in 1st quadrant of unit circle
    approx_array = ((x**2) + (y**2))<= 1 
    # mean of array approximates proportion of unit square and unit circle - pi/4
    approx = approx_array.mean()          
    # 95% confidence interval for estimate
    ci = st.norm.interval(alpha=0.95, loc=np.mean(4*approx_array),scale=st.sem(4*approx_array)) 
    # return estimate of pi
    return (4*approx), ci                

# test cases
print("The pi approximation with a sample size of 1000 is", pi_approx(1000)[0])
print("The 95% confidence interval at n=1000 is", pi_approx(1000)[1])
print("The pi approximation with a sample size of 2000 is", pi_approx(2000)[0])
print("The 95% confidence interval at n=2000 is", pi_approx(2000)[1])
print("The pi approximation with a sample size of 4000 is", pi_approx(4000)[0])
print("The 95% confidence interval at n=4000 is", pi_approx(4000)[1])
print("The pi approximation with a sample size of 8000 is", pi_approx(8000)[0])
print("The 95% confidence interval at n=8000 is", pi_approx(8000)[1])

```


## Q5

Find the first 10-digit prime number occurring in consecutive digits of pi.


Solution from Lunetta Giovanni.

```{python}
# importing these libraries allows us to use arbitrary-precision decimal 
# arithmetic and prime numbers
import decimal
from sympy import isprime
# defines the proper function
def find_10_digit_prime_in_e():
    # sets the precision of the decimal module to 10000, 
    # meaning that we will be working with 10000 decimal digits of e
    decimal.getcontext().prec = 10000
    # use the built-in function in python's decimal module to compute
    #  the value of e and store it in a variable e
    e = decimal.Decimal(1).exp()
    # convert the value of e to a string
    e = str(e)
    # remove the decimal point from the string representation of e
    e = e.replace(".", "")
    # starts a for loop that will run once for each possible starting 
    # index of a 10-digit sequence in the string e
    for i in range(len(e) - 10):
        # creates a variable named num that is set 
        # to the 10-digit sequence
        # starting at index i of the string e
        num = int(e[i:i+10])
        # if the number is prime it returns the number
        if isprime(num):
            return num
print(find_10_digit_prime_in_e())
print("Help from:")
print("https://stackoverflow.com/")
print("https://math.stackexchange.com/")
```



## Q6 NYC Crash data preparation


Solutions from Bedard Kaitlyn.

### Question 1
Importing January 2023 NYC crash data.
```{python}
import pandas as pd

# import data
jan23 = pd.read_csv("./data/nyc_crashes_202301.csv")
# create a copy for cleaning
jan23_cleaning = jan23.copy()
# gives the dimensions of our data
jan23.shape
# list containing our variables
vars = list(jan23)
```

### Question 2
Our discrete variables are `CRASH DATE`, `CRASH TIME`, `BOROUGH`, `ZIPCODE`, `ON STREET NAME`,
`CROSS STREET NAME`, `OFF STREET NAME`, `CONTRIBUTING FACTOR VEHICLE 1`, `CONTRIBUTING FACTOR VEHICLE 2`,
`CONTRIBUTING FACTOR VEHICLE 3`, `CONTRIBUTING FACTOR VEHICLE 4`, `CONTRIBUTING FACTOR VEHICLE 5`,
`COLLISION_ID`, `VEHICLE TYPE CODE 1`, `VEHICLE TYPE CODE 2`,`VEHICLE TYPE CODE 3`,
`VEHICLE TYPE CODE 4`, `VEHICLE TYPE CODE 5`.
We do not include `COLLISION_ID` because each ID is unique, and we do not include
`CONTRIBUTING FACTOR VEHICLE 1` because it will be discussed in question 7.
The output of the code consists of the percent of missing entries for each variable, 
the descriptive statistics for the continuous variables, and the frequency tables for the discrete variables. 
```{python}
# percent missing for each variable
print((jan23.isnull().sum() * 100)/ len(jan23))

# descriptive statistics for continuous variables
print(jan23.describe())

# frequency tables for discrete variables
print(jan23["CRASH DATE"].value_counts(dropna=False))
print(jan23["CRASH TIME"].value_counts(dropna=False))
print(jan23["BOROUGH"].value_counts(dropna=False))
print(jan23["ZIP CODE"].value_counts(dropna=False))
print(jan23["ON STREET NAME"].value_counts(dropna=False))
print(jan23["CROSS STREET NAME"].value_counts(dropna=False))
print(jan23["OFF STREET NAME"].value_counts(dropna=False))
  #jan23["CONTRIBUTING FACTOR VEHICLE 1"].value_counts(dropna=False)
print(jan23["CONTRIBUTING FACTOR VEHICLE 2"].value_counts(dropna=False))
print(jan23["CONTRIBUTING FACTOR VEHICLE 3"].value_counts(dropna=False))
print(jan23["CONTRIBUTING FACTOR VEHICLE 4"].value_counts(dropna=False))
print(jan23["CONTRIBUTING FACTOR VEHICLE 5"].value_counts(dropna=False))
print(jan23["CONTRIBUTING FACTOR VEHICLE 1"].value_counts(dropna=False))
print(jan23["VEHICLE TYPE CODE 1"].value_counts(dropna=False))
print(jan23["VEHICLE TYPE CODE 2"].value_counts(dropna=False))
print(jan23["VEHICLE TYPE CODE 3"].value_counts(dropna=False))
print(jan23["VEHICLE TYPE CODE 4"].value_counts(dropna=False))
print(jan23["VEHICLE TYPE CODE 5"].value_counts(dropna=False))
```

### Question 3
For the most part the values of `LATITUDE` and `LONGITUDE` all look legitimate, 
however there are a handful of entries -- 76 -- that are 0.000000, which is not possible
considering NYC is not located at this longitude or latitude. The below code shows
the frequency tables for these two variables, and it then changes the entries
with 0.0s to missing values.

```{python}
import numpy as np
# frequency tables 
print(jan23["LATITUDE"].value_counts(dropna=False))
print(jan23["LONGITUDE"].value_counts(dropna=False))
# replace 0.0 with nan (on cleaning dataframe)
jan23_cleaning["LATITUDE"] = jan23["LATITUDE"].replace([0.0], np.nan)
jan23_cleaning["LONGITUDE"] = jan23["LONGITUDE"].replace([0.0], np.nan)
```

### Question 4
There were 32 instances of missing latitude and longitude in which there was 
an entry for `OFF STREET NAME`. The below code finds any instance in which 
`OFF STREET NAME` is not missing and `LATITUDE` and `LONGITUDE` are missing, 
and replaces the missing location via geocoding (if the geocoding method 
is able to locate it). 

```{python}
from geopy.geocoders import Nominatim
import numpy as np

# determines the latitude and longitude given a street address
def location(address):
    # concatenate more information
    new_addr = address + ", New York, New York"
    # try to locate address
    try:
        geolocator = Nominatim(user_agent="my_request")
        location = geolocator.geocode(new_addr)
        return location.latitude, location.longitude
    # if cannot locate, return nan
    except:
        return np.nan, np.nan

# all crashes that have an off street name entry
has_off_street = jan23['OFF STREET NAME'].notnull()
# all crashes with no latitude entry
no_lat = jan23['LATITUDE'].isnull()
# all crashes with an off street entry but no latitude entry
has_street_no_lat = jan23_cleaning[has_off_street & no_lat]


count = 0
# for each entry 
for i in has_street_no_lat["OFF STREET NAME"]:
    # if location is found, update the latitude and longitude
    if location(i) is not None:
        lat, lng = location(i)
        jan23_cleaning["LATITUDE"][count] = lat  
        jan23_cleaning["LONGITUDE"][count] = lng
        # printing to show the updates
        print(jan23_cleaning["LATITUDE"][count],jan23_cleaning["LONGITUDE"][count])
    count+=1

```

### Question 5 (optional)
From the below code, we see that there are 32 reports that are missing both latitude
and on street name. There are many more reports missing the on street name input 
then those missing the latitude. However, not all reports missing on street name
are missing latitude, and vice versa. Thus, the missing patterns of these two 
variables do not exactly match. We also see that there are 209 reports that have
both an input for cross street name and on street name, but do not have have 
latitude and longitude inputs. We could theoretically determine the geolocation 
based on this information and fill in these missing values. (However, I could 
not figure out how to do so)

* Note: I am working on original data frame, not the one altered in the last question.
```{python}
# set with indexes of all crashes missing "ON STREET NAME"
no_onstreet = set(jan23[jan23['ON STREET NAME'].isnull()].index.tolist())
# set with indexes of all crashes not missing "ON STREET NAME"
has_onstreet = set(jan23[jan23['ON STREET NAME'].notnull()].index.tolist())
# set with indexes of all crashes missing "LATITUDE"
no_lat = set(jan23[jan23['LATITUDE'].isnull()].index.tolist())
# set with indexes of all crashes not missing "LATITUDE"
has_lat = set(jan23[jan23['LATITUDE'].notnull()].index.tolist())

# set with indexes of crashes missing on street and latitude
missing_both = no_onstreet.intersection(no_lat)
print("Number of crashes missing both on street name and latitude:" , len(missing_both))

# cross table for on street and latitude
with pd.option_context('display.max_rows', None): 
  pd.crosstab(index = jan23["ON STREET NAME"], columns = jan23["LATITUDE"], dropna = False)

# set with indexes of all crashes not missing cross street name
has_cross_street = set(jan23[jan23['CROSS STREET NAME'].notnull()].index.tolist())
# set with indexes of crashes that have a cross street name and on street name
cross_and_on = has_onstreet.intersection(has_cross_street)

# set with indexes containing the off street and cross street but missing geolocation
cross_on_nolat = cross_and_on.intersection(no_lat)
print("Number of crashes with both on street name and cross street name but missing latitude and longitude:" ,len(cross_on_nolat))
```

### Question 6
Yes, zip code and borough are always missing together. The below code 
uses reverse geocoding to fill in the missing zips and boroughs, given 
that the latitude and longitude are available for a given crash. See 
the output for the replaced zipcodes and boroughs.
```{python}
# series with all crashes missing "ZIP CODE"
no_zip = jan23['ZIP CODE'].isnull()

# series with all crashes missing "BOROUGH"
no_borough = jan23['BOROUGH'].isnull()

# check if zipcode and borough are always missing together
print("Zip and Borough are always missing together? ", no_zip.equals(no_borough))

# creates a series containing reports that are missing zipcode but have latitude
no_zip_yes_lat_mask = no_zip & (jan23["LATITUDE"].notnull())
no_zip_yes_lat = jan23[no_zip_yes_lat_mask]

# imports for the following functions
from uszipcode import SearchEngine
import numpy as np
from typing import Union, List

# Note this is code from the class notes
# Given zipcode, computes the respective borough
def nyczip2borough(zips: pd.Series) -> pd.Series:
    zips = zips.values if isinstance(zips, pd.Series) else zips
    condlist = [
        (zips >= 10001) & (zips <= 10282),
        (zips >= 10301) & (zips <= 10314),
        (zips >= 10451) & (zips <= 10475),
        (zips >= 11004) & (zips <= 11109),
        (zips >= 11351) & (zips <= 11697),
        (zips >= 11201) & (zips <= 11256),
    ]
    choicelist = [
        "MANHATTAN",
        "STATEN ISLAND",
        "BRONX",
        "QUEENS",
        "QUEENS",
        "BROOKLYN",
    ]
    result = pd.Series(np.select(condlist, choicelist, default=np.nan))
    return result

sr = SearchEngine()
# find the missing zipcodes
zipcodes = [(int(sr.by_coordinates(lat, lng, radius=5)[0].zipcode)) for lat, lng in zip(no_zip_yes_lat['LATITUDE'], no_zip_yes_lat['LONGITUDE'])]
zipcodes = pd.Series(zipcodes)

# find the missing boroughs
boroughs = nyczip2borough(zipcodes)

# replace missing entry with found zipcode
jan23_cleaning.loc[no_zip_yes_lat_mask, "ZIP CODE"] = zipcodes.values
# replace missing entry with found borough
jan23_cleaning.loc[no_zip_yes_lat_mask, "BOROUGH"] = boroughs.values

# printing to show that it works
print(jan23_cleaning.loc[no_zip_yes_lat_mask, "ZIP CODE"])
print(jan23_cleaning.loc[no_zip_yes_lat_mask, "BOROUGH"])

```

### Question 7
The below code prints the whole frequency table of 
`CONTRIBUTING FACTOR VEHICLE 1` and then converts 
the entries to all lowercase and checks the frequencies again.
There is no difference. 
```{python}
with pd.option_context('display.max_rows', None):
  print(jan23["CONTRIBUTING FACTOR VEHICLE 1"].value_counts(dropna=False))

with pd.option_context('display.max_rows', None):
  print(jan23["CONTRIBUTING FACTOR VEHICLE 1"].str.lower().value_counts(dropna=False))
```

### Question 8
Given an opportunity to meet with the data provider, I would give them the following suggestions:

*  It seems unneeded to include location, given that latitude and longitude is already included

*   Suggest that the possible entries for contributing factors and vehicle types are standardizes

*   Suggest that they use given information (such as latitude and longitude) to fill out zipcodes, boroughs, or street addresses, or vice versa to make the data more complete

*   Clarify whether a crash at 00:00 is actually at midnight or if it should be treated as a missing value


## Q7

Solutions from Bedard Kaitlyn.

### Question 1
The below code constructs a contingency table for missing location by borough, and then performs a 
hypothesis test to see if the pattern is the same across all boroughs.
The null hypothesis we will test is: the proportion of missing locations (ie: the missing pattern) is 
the same across all boroughs. The alternative hypothesis is: the porportion of missing locations 
(the missing pattern) is different across all boroughs. We will preform a chi-squared test at a 
significance level of 0.05. Based on the below results, we do not have evidence to reject the null 
hypothesis, as the p-value of the test is 0.13, thus, we can say the pattern of missing locations is 
roughly the same across all boroughs. 
```{python}
import pandas as pd
import numpy as np
# import data (uncleaned)
jan23 = pd.read_csv("./data/nyc_crashes_202301.csv")
# import cleaned data
jan23_clean = pd.read_csv("./data/nyc_crashes_202301_cleaned.csv")
# Construct and print the contingenxy table 
contingency_table = pd.crosstab(index=jan23['LOCATION'].isnull(), columns=jan23['BOROUGH'])
print(contingency_table)
# import needed packages for testing
from scipy.stats import chi2_contingency
# run the test
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)
# Print the results
print("Chi-squared statistic: ", chi2_stat)
print("p-value: ", p_value)
```

### Question 2
The below code constructs a new variable, `HOUR`, with integers ranging from 0 to 23. It then plots a 
histogram of the number of crashes by hour, as well as histograms of crashes by hour for each borough. 
See the files in the repo to view the histograms. (Trouble getting the images to be shown in the release.)
```{python}
import matplotlib.pyplot as plt
# Split the crash time into hours
jan23_clean['HOUR'] = jan23_clean['CRASH TIME'].str.split(':').str.get(0)
jan23_clean['HOUR'] = jan23_clean['HOUR'].astype(int)
#(Commenting out the plot set up to avoid printing)
# Plot the histogram of crash times by hour
#jan23_clean.hist(column="HOUR", bins=24)
 
# Set the x and y labels
plt.title("Number of Crashes by Hour")
plt.xlabel("Hour of Day")
plt.ylabel("Number of Crashes")
# Save the plot as a PNG file
plt.savefig("crashes_by_hour.png")
# Plot the histograms of crash times by hour and borough
jan23_clean.hist(column="HOUR", bins=24, by="BOROUGH")
# Set the x and y labels for each subplot
for ax in plt.gcf().axes:
   ax.set_xlabel("Hour of Day")
   ax.set_ylabel("Number of Crashes")
plt.tight_layout()
plt.show()
filename = f"crashes_by_hour_by_borough.png"
plt.savefig(filename)
plt.close()
```



### Question 3
The below code plots all the crashes (that have latitude and longitude information available) 
on an interactive map. See the other release to view the map. Scroll in to see the specific markers
of each crash in a given area. 
```{python}
# using folium to make graph (couldn't get google maps to work)
import folium
from folium.plugins import MarkerCluster
from IPython.display import IFrame
# Create a map centered on New York City
nyc_map = folium.Map(location=[40.7, -74.0], zoom_start=10)
# Create a marker cluster to hold the data points
marker_cluster = MarkerCluster().add_to(nyc_map)
# Add each data point to the marker cluster
for index, row in jan23_clean.iterrows():
  if not pd.isna(row['LATITUDE']):
    folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']]).add_to(marker_cluster)
# Save the map as an HTML file
map_file = "nyc_map.html"
nyc_map.save(map_file)
```

### Question 4
The below code constructs a new variable `injury` which is 1 if the number of persons injuried is 1 or more
and 0 otherwise. It then constructs a contingency table for injury versus borough, and conducts a hypothesis test.
The null hypothesis we will test is that that injury and borough are not associated, meaning that each 
borough has the same porportion of injuries. The alternative hypothesis would be that injury and borough 
are associated, meaning that the pattern of injury differs across the boroughs. We preform a chi-squared 
test, at a significance level of 0.05. Based on the below results, a p-value of 0.006, we have evidence to reject 
the null hypothesis. Thus, we can say that the number of injuries is not the same among all the boroughs, therefore
the two variables injury and borough are associated.
```{python}
# create new  variable, injury
jan23_clean['injury'] = (jan23_clean['NUMBER OF PERSONS INJURED'] > 1).astype(int)
# construct and print cross table
contingency_table = pd.crosstab(index=jan23_clean['injury'], columns=jan23_clean['BOROUGH'])
print(contingency_table)
# run the chi-squared test
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)
# Print the results
print("Chi-squared statistic: ", chi2_stat)
print("p-value: ", p_value)
```

### Question 5
The below code merges the zip code database with the crash data, using the `uszipcode` library.
To avoid redundant information in the merge, I specified which variables from the zip code database
to merge with the crash data. See the head of the merged dataset to see the new variables.
```{python}
# import zipcode package
from uszipcode import SearchEngine
sr = SearchEngine()
# convert zipcodes to strings
jan23_clean["ZIP CODE"] = jan23_clean["ZIP CODE"].replace(np.nan, 0)
jan23_clean["ZIP CODE"] = jan23_clean["ZIP CODE"].astype(int)
jan23_clean["ZIP CODE"] = jan23_clean["ZIP CODE"].astype(str)
jan23_clean["ZIP CODE"] = jan23_clean["ZIP CODE"].replace('0', np.nan)
# for each zip code in the dataset, find corresponding zip info
for i, zipcode in enumerate(jan23_clean["ZIP CODE"]):
    if not pd.isna(zipcode):
      result = sr.by_zipcode(zipcode)
      # adding only certain variables from the uszipcode info (to avoid redundancy)
      jan23_clean.loc[i, 'zip_radius'] = result.radius_in_miles 
      jan23_clean.loc[i, 'zip_population'] = result.population
      jan23_clean.loc[i, 'zip_population_density'] = result.population_density 
      jan23_clean.loc[i, 'zip_land_area'] = result.land_area_in_sqmi 
      jan23_clean.loc[i, 'zip_water_area'] = result.water_area_in_sqmi 
      jan23_clean.loc[i, 'zip_housing_units'] = result.housing_units
      jan23_clean.loc[i, 'zip_home_value'] = result.median_home_value 
      jan23_clean.loc[i, 'zip_income'] = result.median_household_income
# show head of data to show successful merge
jan23_clean.head()
```

### Question 6
We fit a logistic regression with the following covariates, obtained from the merge with the zipcode data: 
`zip_population_density`, `zip_home_value`, and `zip_income`. From the below results, it is safe to say 
that the model is not a good fit, based on the high log-likelihood, pearson chi2 value, and deviance, as
well as the low R-squared value. However, based on the p-values associated with each parameter, in this 
model, the median home value of the zipcode that a crash occurs is statistically significant (p = 0.009). 
This implies that there may be some relationship between the median home value of the area and whether or 
not injuries occur during a crash. The coefficient of this variable is -2.624, so this implies that as the 
home values increase, it is less likely that a crash results in an injury. This is an interesting observation 
that could imply that wealthier neighborhoods are also safer for driving. The other covariates used in this 
logistic regression however -- population density and median income of the zipcode of th crash --  were not 
found to be statistically significant in this model.
```{python}
# import needed packages
import statsmodels.api as sm
import statsmodels.formula.api as smf
# construct the logistic regression model
mylogistic = smf.glm(formula = 'injury ~ zip_population_density + zip_home_value + zip_income', data = jan23_clean,
                     family = sm.families.Binomial())
# print logistic fit results
mylfit = mylogistic.fit()
mylfit.summary()
```


## Q8


Solution from Bedard Kaitlyn.

```{python}
import pandas as pd
# loading in the cleaned data
jan23 = pd.read_csv("./data/nyc_crashes_zip_merge.csv")
# calculating number of injuries
jan23['sum'] = jan23['NUMBER OF PERSONS INJURED'] + jan23['NUMBER OF PEDESTRIANS INJURED']+ jan23['NUMBER OF CYCLIST INJURED'] + jan23['NUMBER OF MOTORIST INJURED']
for index in jan23.index:
    if jan23['sum'][index] > 0:
        jan23.loc[index,['injured']] = 1
    else:
        jan23.loc[index,['injured']] = 0
jan23.head()
```

### Part 1 - Parameters
I chose to include the following parameters in the fitting of both the logistic regression and SVM models. 
These are the same parameters I selected last week as well. They were chosen mainly because, intuitively, 
it would make sense that these parameters may affect the `injured` variable. 
 
* `zip_population_density`: the population density of the zipcode in which a crash occured

* `zip_home_value`: the median home value of the zipcode in which a crash occured

* `zip_income`: the median household income of the zipcode in which a crash occured


### Part 2 - Confusion Matrix Results
For the logistic regression model, our model correctly predicted 948 instances of no injury in a crash, however it did not predict correctly 567 crashes that resulted in injury. 

For the logistic regression model, our model correctly predicted 121 instances of no injury in a crash, however it did not predict correctly 69 crashes that did result in injury. 

Essentially, for both models, the confusion matrices indicate that the model does not make accurate predictions - especially at predicting a crash with an injury.

**Creating Training and Testing Data for Logistic Regression**
```{python}
# test data is the last week of January
test = jan23[jan23["CRASH DATE"].str.contains('01/25/2023|01/26/2023|01/27/2023|01/28/2023|01/29/2023|01/30/2023|01/31/2023')]
# training data is everything else
train = jan23[~jan23["CRASH DATE"].str.contains('01/25/2023|01/26/2023|01/27/2023|01/28/2023|01/29/2023|01/30/2023|01/31/2023')]
# select the parameters to be included in the fit 
train_focus_data = train[['zip_population_density','zip_home_value', 'zip_income','injured']]
# drop NaN rows to combat errors with fitting
training = train_focus_data.dropna()
# select the parameters to be included in the testing
test_focus_data = test[['zip_population_density','zip_home_value', 'zip_income','injured']]
# drop NaN rows
testing = test_focus_data.dropna()
# y training and testing
y_train = training['injured'].values
y_test = testing['injured'].values
# X training and testing
X_train = training.drop(labels = ['injured'], axis=1)
X_test = testing.drop(labels = ['injured'], axis=1)
```

**Logistic Regression**
```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
# Define the logistic regression model
logreg = LogisticRegression(solver='liblinear')
# Define the grid for tuning parameters
param_grid = {'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}
# Use cross validation to find the best tuning parameters
grid_search = GridSearchCV(logreg, param_grid, cv=5)
grid_search.fit(X_train, y_train)
# print the best tuning parameters
print("Best hyperparameters: {}".format(grid_search.best_params_))
# logistic regression with the best tuning parameters
logreg_best = grid_search.best_estimator_
from sklearn.metrics import confusion_matrix
# Fit the logistic regression model with best hyperparameters on the training data
logreg_best.fit(X_train, y_train)
# Make predictions
y_pred_log = logreg_best.predict(X_test)
# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred_log)
print('Logistic Regression Confusion Matrix:')
print(cm)
```


**Creating Training and Testing Data for SVM**
```{python}
import numpy as np
# test data is a random sample of 200 crashes
test_svm = jan23.sample(n=200, random_state=1)
# training data is a random sample of 800 crashes
train_svm = jan23.drop(test.index).sample(n=800, random_state=1)
# select the parameters to be included in the fit 
train_focus_data_svm = train_svm[['zip_population_density', 'zip_home_value', 'zip_income', 'injured']]
# drop NaN rows to combat errors with fitting
training_svm = train_focus_data_svm.dropna()
# select the parameters to be included in the testing
test_focus_data_svm = test_svm[['zip_population_density', 'zip_home_value', 'zip_income', 'injured']]
# drop NaN rows
testing_svm = test_focus_data_svm.dropna()
# y training and testing
y_train_svm = training_svm['injured'].values
y_test_svm = testing_svm['injured'].values
# X training and testing
X_train_svm = training_svm.drop(labels=['injured'], axis=1)
X_test_svm = testing_svm.drop(labels=['injured'], axis=1)
```
**Support Vector Machines**
```{python}
from sklearn import svm
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
# Define the SVM model
svc = svm.SVC()
# Define the hyperparameters to search over
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly']}
# Use grid search cross-validation to find the best hyperparameters
grid_search = GridSearchCV(svc, param_grid=param_grid, scoring='roc_auc')
grid_search.fit(X_train_svm, y_train_svm)
# Print the best hyperparameters
print('Best hyperparameters:', grid_search.best_params_)
# Fit the SVM model using the best hyperparameters
best_svc = svm.SVC(**grid_search.best_params_)
best_svc.fit(X_train_svm, y_train_svm)
# Make predictions on the test set
y_pred_svm = best_svc.predict(X_test_svm)
# Compute confusion matrix
cm = confusion_matrix(y_test_svm, y_pred_svm)
print('SVM Confusion Matrix:')
print(cm)
```

### Part 3 - Performance Comparisons
From the below results, it is clear that neither approach is able to predict injury well. This is not suprising considering we consider only a few parameters. Both models give a precision, recall, and F1-score of 0.0. These results essentially mean that neither model did a good job identifying the crashes that resulted in injuries. We see an accuracy of 0.6257 and an AUC of 0.5 for the logistic model and an accuracy of 0.6368 and an AUC of 0.5 for the SVM model. So, our SVM is very slightly more accurate than the logistic model. It is also important to note that the SVM model is trained with a much smaller sample than the logistic model (due to the fact the SVM training is time consuming for large samples.) So, SVM would most likely be a better approach in modeling the fit, especially if we use a larger sample size.

**Logistic Regression Results**
```{python}
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
# Compute the accuracy score
accuracy = accuracy_score(y_test, y_pred_log)
print('Accuracy:', accuracy)
# Compute the precision score
precision = precision_score(y_test, y_pred_log)
print('Precision:', precision)
# Compute the recall score
recall = recall_score(y_test, y_pred_log)
print('Recall:', recall)
# Compute the F1-score
f1 = f1_score(y_test, y_pred_log)
print('F1-score:', f1)
# Compute the AUC score
#y_pred_prob = logreg_best.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test, y_pred_log)
print('AUC:', auc)
```

**SVM Results**
```{python}
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
# Compute the accuracy score
accuracy = accuracy_score(y_test_svm, y_pred_svm)
print('Accuracy:', accuracy)
# Compute the precision score
precision = precision_score(y_test_svm, y_pred_svm)
print('Precision:', precision)
# Compute the recall score
recall = recall_score(y_test_svm, y_pred_svm)
print('Recall:', recall)
# Compute the F1-score
f1 = f1_score(y_test_svm, y_pred_svm)
print('F1-score:', f1)
# Compute the AUC score
#y_pred_prob = logreg_best.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test_svm, y_pred_svm)
print('AUC:', auc)
```